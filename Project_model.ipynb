{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OldBaseline_KLL_MLT22 Git revision: model.py (EXPLANATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below we have the whole model code. Let's break down it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "# import setGPU\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.use('agg')\n",
    "\n",
    "import pickle\n",
    "from autoencoder_classes import AE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from custom_layers import Sampling\n",
    "\n",
    "from qkeras import QDense, QActivation, QBatchNormalization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras\n",
    "\n",
    "\n",
    "def build_AE(input_shape,latent_dim):\n",
    "    \n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # encoder = LeakyReLU(alpha=0.3)(x)\n",
    "    #decoder\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "    #create autoencoder\n",
    "    autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "    autoencoder.summary()\n",
    "    # ae = AE(autoencoder)\n",
    "    # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "def build_VAE_orig(input_shape,latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    # vae = VAE(encoder, decoder)\n",
    "    # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    return encoder,decoder\n",
    "\n",
    "def build_VAE(input_shape,latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = Dense(32, kernel_initializer='lecun_uniform')(inputArray)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(16, kernel_initializer='lecun_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer='zeros')(x)\n",
    "    logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer='zeros')(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    y = Dense(16,kernel_initializer='lecun_uniform')(d_input)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(32,kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(64,kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(128,kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    dec = Dense(input_shape, kernel_initializer='lecun_uniform')(y)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    # vae = VAE(encoder, decoder)\n",
    "    # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    return encoder,decoder\n",
    "\n",
    "def build_VAE_nobn(input_shape,latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = Dense(32, kernel_initializer='lecun_uniform')(inputArray)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(16, kernel_initializer='lecun_uniform')(x)\n",
    "    x = ReLU()(x)\n",
    "    mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer='zeros')(x)\n",
    "    logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer='zeros')(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    y = Dense(16,kernel_initializer='lecun_uniform')(d_input)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(32,kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(64,kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(128,kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    dec = Dense(input_shape, kernel_initializer='lecun_uniform')(y)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    # vae = VAE(encoder, decoder)\n",
    "    # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    return encoder,decoder\n",
    "\n",
    "    \n",
    "def build_QVAE(input_shape,latent_dim,quant_size=12,integer=4,symmetric=0,pruning='pruned',batch_size=1024):\n",
    "\n",
    "    quant_size = 12\n",
    "    integer = 4\n",
    "    symmetric = 0\n",
    "    pruning='pruned'\n",
    "\n",
    "    if pruning == 'pruned':\n",
    "        ''' How to estimate the enc step:\n",
    "                num_samples = input_train.shape[0] * (1 - validation_split)\n",
    "                end_step = np.ceil(num_samples / batch_size).astype(np.int32) * pruning_epochs\n",
    "                so, stop pruning at the 7th epoch\n",
    "        '''\n",
    "        begin_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*5\n",
    "        end_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*15\n",
    "        print('Begin step: ' + str(begin_step) + ', End step: ' + str(end_step))\n",
    "        \n",
    "        pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "                                initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                                begin_step=begin_step, end_step=end_step)\n",
    "        print(pruning_schedule.get_config())\n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = QActivation(f'quantized_bits(16,10,0,alpha=1)')(inputArray)\n",
    "    x = QBatchNormalization()(x)\n",
    "    x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +'), alpha=1',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    mu = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)))(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    logvar = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')    \n",
    "    encoder.summary()\n",
    "\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Statements** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "#import setGPU\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#matplotlib.use('agg')\n",
    "\n",
    "import pickle\n",
    "from autoencoder_classes import AE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from custom_layers import Sampling\n",
    "\n",
    "from qkeras import QDense, QActivation, QBatchNormalization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Key task: Research about all the packages and dependencies that were called. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function Definition**\n",
    "Now we will define a classical autoencoder structure that is an alternative to carry out the anomaly detection.\n",
    "In this context, we will see four classical AE (including VAE). In summary, we will notice that the following differences in functionality exist:\n",
    "* Autoencoder (`build_AE`):\n",
    "    * Focuses on learning an efficient encoding for input data.\n",
    "    * Primarily used for reconstructing input data and identifying anomalies based on reconstruction error.\n",
    "* Variational Autoencoder (`build_VAE_orig`, `build_VAE`, `build_VAE_nobn`):\n",
    "    * Adds a probabilistic layer by encoding inputs into a distribution rather than a fixed vector.\n",
    "    * Useful for generating new data points and anomaly detection by assessing how well data fits into the learned distribution.\n",
    "    * `build_VAE_orig` includes Batch Normalization and LeakyReLU.\n",
    "    * `build_VAE` modifies the architecture with different initializers, ReLU, and additional layers.\n",
    "    * `build_VAE_nobn` further simplifies the model by removing Batch Normalization.\n",
    "    \n",
    "Each model serves a slightly different purpose and might be chosen based on specific requirements of the anomaly detection task, such as computational efficiency, model complexity, and the nature of the data being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***build_AE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_AE(input_shape, latent_dim):\n",
    "    \n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "    #decoder\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "    #create autoencoder\n",
    "    autoencoder = Model(inputs=inputArray, outputs=decoder)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure**:\n",
    "\n",
    "    * ***Input***: Takes an input shape and passes it through the network. `inputArray = Input(shape=(input_shape))` defines the input shape for the model.\n",
    "    * ***Encoder***:\n",
    "        * Batch normalization\n",
    "        * Dense layer (32 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer (16 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer for encoding to latent space (latent_dim units, HeUniform initializer). It encodes the input into a latent space of dimension latent_dim.\n",
    "   * ***Decoder***: Mirrors the encoder to reconstruct the input. That is, it reverses the encoder operation.\n",
    "        * Dense layer (16 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer (32 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer to reconstruct the input (same shape as input, HeUniform initializer). That is, it decodes back to the original input shape.\n",
    "\n",
    "* **Functionality**:\n",
    "\n",
    "    * The autoencoder compresses the input data into a smaller representation (latent space) and then reconstructs the original data from this representation. This is useful for capturing the most important features of the data and for anomaly detection by measuring reconstruction errors. It combines encoder and decoder into a single model and prints the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Key question: What is the latent space? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***build_VAE_orig***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_VAE_orig(input_shape, latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    mu = Dense(latent_dim, name='latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    logvar = Dense(latent_dim, name='latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure**:\n",
    "\n",
    "    * ***Input***: Takes an input shape and processes it through the network.\n",
    "    * ***Encoder***:\n",
    "        * Batch normalization\n",
    "        * Dense layer (32 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer (16 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layers for mean (mu) and log variance (logvar) of latent space (latent_dim units, HeUniform initializer)\n",
    "        * Sampling layer to reparameterize (z)\n",
    "   * ***Decoder***:\n",
    "        * Dense layer (16 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer (32 units, HeUniform initializer)\n",
    "        * Batch normalization\n",
    "        * LeakyReLU activation\n",
    "        * Dense layer to reconstruct the input (same shape as input, HeUniform initializer)\n",
    "        \n",
    "* **Functionality**:\n",
    "    * The VAE adds a probabilistic twist to the autoencoder by encoding the input into a distribution defined by `mu` and `logvar`. The Sampling layer uses the reparameterization trick to ensure proper gradient flow during training. The decoder reconstructs the input from samples drawn from this latent distribution. This helps in generating new data samples and handling anomalies by comparing the probabilistic reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Key question: How is this probabilistic twist? What is the nature of mu and logvar? How were they initialized and why? How it changes the nature of an AE? What reparameterization trick was used? How the Sampling Layer works? What does proper gradient flow mean? How is this latent distribution? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***build_VAE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_VAE(input_shape, latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = Dense(32, kernel_initializer='lecun_uniform')(inputArray)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(16, kernel_initializer='lecun_uniform')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    mu = Dense(latent_dim, name='latent_mu', kernel_initializer='zeros')(x)\n",
    "    logvar = Dense(latent_dim, name='latent_logvar', kernel_initializer='zeros')(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    y = Dense(16, kernel_initializer='lecun_uniform')(d_input)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(32, kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(64, kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(128, kernel_initializer='lecun_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    dec = Dense(input_shape, kernel_initializer='lecun_uniform')(y)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure**:\n",
    "\n",
    "    * ***Input***: Takes an input shape and processes it through the network.\n",
    "    * ***Encoder***:\n",
    "        * Dense layer (32 units, lecun_uniform initializer)\n",
    "        * Batch normalization\n",
    "        * ReLU activation\n",
    "        * Dense layer (16 units, lecun_uniform initializer)\n",
    "        * Batch normalization\n",
    "        * ReLU activation\n",
    "        * Dense layers for mean (mu, initialized with zeros) and log variance (logvar, initialized with zeros) of latent space (latent_dim units)\n",
    "        * Sampling layer to reparameterize (z)\n",
    "    * ***Decoder***:\n",
    "        * Dense layer (16 units, lecun_uniform initializer)\n",
    "        * Batch normalization\n",
    "        * ReLU activation\n",
    "        * Dense layer (32 units, lecun_uniform initializer)\n",
    "        * Batch normalization\n",
    "        * ReLU activation\n",
    "        * Additional dense layers (64 and 128 units, lecun_uniform initializer)\n",
    "        * Batch normalization and ReLU activations\n",
    "        * Dense layer to reconstruct the input (same shape as input, lecun_uniform initiailizer)\n",
    "\n",
    "* **Functionality**:\n",
    "\n",
    "    * Similar to build_VAE_orig, but with several modifications:\n",
    "        * Uses `lecun_uniform` initializers.\n",
    "        * More layers in the decoder for potentially better reconstruction.\n",
    "        * `ReLU` activations instead of `LeakyReLU`. \n",
    "        * `mu` and `logvar` initialized with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Key question: What is the different between lecun_uniform and HeUnfiorm initializers? Why lecun_uniform was chosen in this case? Why ReLU is preferred over LeakyRELU in this case? Why were mu and logvar initialized with zeros?  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***build_VAE_nobn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_VAE_nobn(input_shape, latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = Dense(32, kernel_initializer='lecun_uniform')(inputArray)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(16, kernel_initializer='lecun_uniform')(x)\n",
    "    x = ReLU()(x)\n",
    "    mu = Dense(latent_dim, name='latent_mu', kernel_initializer='zeros')(x)\n",
    "    logvar = Dense(latent_dim, name='latent_logvar', kernel_initializer='zeros')(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    y = Dense(16, kernel_initializer='lecun_uniform')(d_input)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(32, kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(64, kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(128, kernel_initializer='lecun_uniform')(y)\n",
    "    y = ReLU()(y)\n",
    "    dec = Dense(input_shape, kernel_initializer='lecun_uniform')(y)\n",
    "\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Structure**:\n",
    "    * ***Input***: Takes an input shape and processes it through the network.\n",
    "    * ***Encoder***:\n",
    "        * Dense layer (32 units, lecun_uniform initializer)\n",
    "        * ReLU activation\n",
    "        * Dense layer (16 units, lecun_uniform initializer)\n",
    "        * ReLU activation\n",
    "        * Dense layers for mean (mu, initialized with zeros) and log variance (logvar, initialized with zeros) of latent space (latent_dim units)\n",
    "        * Sampling layer to reparameterize (z)\n",
    "    * ***Decoder***:\n",
    "        * Dense layer (16 units, lecun_uniform initializer)\n",
    "        * ReLU activation\n",
    "        * Dense layer (32 units, lecun_uniform initializer)\n",
    "        * ReLU activation\n",
    "        * Additional dense layers (64 and 128 units, lecun_uniform initializer)\n",
    "        * ReLU activations\n",
    "        * Dense layer to reconstruct the input (same shape as input, lecun_uniform initializer)\n",
    "* **Functionality**:\n",
    "    * Similar to build_VAE, but without Batch Normalization layers.\n",
    "    * Simplifies the model, potentially reducing training time and computational resources, at the expense of some benefits provided by Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Key question: How much is the influence of don't use Batch Normalization? Why is it preferred? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***build_QVAE***\n",
    "\n",
    "This function builds a Quantized VAE with optional pruning of less significant weights to achieve a sparse model. This function combines concepts from VAE, quantization, and pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_QVAE(input_shape,latent_dim,quant_size=12,integer=4,symmetric=0,pruning='pruned',batch_size=1024):\n",
    "\n",
    "    quant_size = 12\n",
    "    integer = 4\n",
    "    symmetric = 0\n",
    "    pruning='pruned'\n",
    "\n",
    "    if pruning == 'pruned':\n",
    "        ''' How to estimate the enc step:\n",
    "                num_samples = input_train.shape[0] * (1 - validation_split)\n",
    "                end_step = np.ceil(num_samples / batch_size).astype(np.int32) * pruning_epochs\n",
    "                so, stop pruning at the 7th epoch\n",
    "        '''\n",
    "        begin_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*5\n",
    "        end_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*15\n",
    "        print('Begin step: ' + str(begin_step) + ', End step: ' + str(end_step))\n",
    "        \n",
    "        pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "                                initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                                begin_step=begin_step, end_step=end_step)\n",
    "        print(pruning_schedule.get_config())\n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = QActivation(f'quantized_bits(16,10,0,alpha=1)')(inputArray)\n",
    "    x = QBatchNormalization()(x)\n",
    "    x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +'), alpha=1',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    mu = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)))(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    logvar = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')    \n",
    "    encoder.summary()\n",
    "\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)\n",
    "    # Create decoder\n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the function line by line for a detailed explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Function Signature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_QVAE(input_shape, latent_dim, quant_size=12, integer=4, symmetric=0, pruning='pruned', batch_size=1024):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `input_shape`: Shape of the input data.\n",
    "* `latent_dim`: Dimensionality of the latent space.\n",
    "* `quant_size`: Number of bits for quantization.\n",
    "* `integer`: Number of integer bits for quantization.\n",
    "* `symmetric`: Symmetry mode for quantization.\n",
    "* `pruning`: Specifies if pruning should be applied.\n",
    "* `batch_size`: Batch size for pruning schedule calculations.\n",
    "\n",
    "<span style=\"color: red;\"> Key question: What do all these parameters mean? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Setting Default Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    quant_size = 12\n",
    "    integer = 4\n",
    "    symmetric = 0\n",
    "    pruning = 'pruned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines reset the quantization and pruning parameters to default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Pruning Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if pruning == 'pruned':\n",
    "        ''' How to estimate the enc step:\n",
    "                num_samples = input_train.shape[0] * (1 - validation_split)\n",
    "                end_step = np.ceil(num_samples / batch_size).astype(np.int32) * pruning_epochs\n",
    "                so, stop pruning at the 7th epoch\n",
    "        '''\n",
    "        begin_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*5\n",
    "        end_step = np.ceil((input_shape*0.8)/batch_size).astype(np.int32)*15\n",
    "        print('Begin step: ' + str(begin_step) + ', End step: ' + str(end_step))\n",
    "        \n",
    "        pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "                                initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                                begin_step=begin_step, end_step=end_step)\n",
    "        print(pruning_schedule.get_config())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, the condition checks if the `pruning` is set to `pruned`. If it is, the code block within the `if` is executed.\n",
    "* The comments at the beginning of the `if` describe a general approach to estimate `begin_step` and `end_step` for pruning. The idea is to stop proning at the 7th epoch.\n",
    "* This block calculates the pruning schedule. `begin_step` and `end_step` are calculated based on the input shape and batch size, and some constants (5 and 15 in this case).\n",
    "* In the definition of `begin_step`, we are using the 80% of the input shape (it might represent 80% of the training data). `np.ceil(...)/batch_size` calculates the number of steps per epoch, and `astype(np.int32)*5` multiplies by 5 to estimate the step to begin prunning.\n",
    "* `end_step`, similarly to `begin_step`, is calculated to be 15 times the number of steps per epoch.\n",
    "* `tfmot.sparsity.keras.PolynomialDecay` defines how pruning progresses over time. It defines a polynomial decay schedule for pruning, sparsity increases gradually following a polynomial curve. `initial_sparsity=0.0` is the sparsity (fraction of zero weights )at the beginning of pruning, and `final_sparsity=0.5` is the target sparsity at the end of pruning. `begin_step` is the step at which pruning begins, and `end_step` at which it ends. This creates a schedule wgere sparsity increases from 0% to 50% in a polynomial manner from `begin_step` to `end_step`. The values `5` and `15` are chosen as multipliers to determine the pruning schedule's start and end points.\n",
    "* The pruning schedule is printed for verification.\n",
    "\n",
    "<span style=\"color: red;\"> Key question: What does sparsity mean? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoder Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = QActivation(f'quantized_bits(16,10,0,alpha=1)')(inputArray)\n",
    "    x = QBatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `inputArray`: Input layer for the encoder with the specified shape by `input_shape`.\n",
    "* `QActivation`: Applies quantized activation. `QActivation(f'quantized_bits(16,10,0,alpha=1)')` applies quantized activation with 16 bits, 10 integer bits, and no symmetric range (`symmetric=0`).\n",
    "* `QBatchNormalization`: Applies batch normalization with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +'), alpha=1',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                                pruning_schedule=pruning_schedule)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note that `\\` in Python is used as a line continuation character. This allows you to split a long line of code into multiple lines for better readability without breaking the logic of the code. Here, we are using the `if` logic conditionales in a not-conventional way.**\n",
    "* Adds a dense (fully connected) layer with optional quantization and pruning.\n",
    "* The code block above refers to the first dense layer with conditional pruning and quantization. If `quant_size == 0`. \n",
    "    * It adds a dense layer with 32 units and HeNormal initializer, wrapped in pruning logic (it would be pruned using the `pruning_schedule`). Then, it specifies the pruning schedule and applies the layer to `x` if `quant_size == 0`.\n",
    "    * The `else` part continues the line, defining the alternative case using a quantized dense layer`QDense` (if `quant_size` is not 0), with specfied kernel and bias quantizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adds batch normalization and activation layers with optional quantization and pruning.\n",
    "* **The first line** applies a Quantized Batch Normalization layer with pruning to `x`. Note that there is no backslash here as the statement fits on a single line.\n",
    "* **The second line**  consists in add Activation Layer with Conditional Pruning and Quantization. Until the `\\`, adds a `ReLU` activation, using `Activation` if `quant_size == 0`. If not, `QActivation` will be used.\n",
    "* The pruning schedule `pruning_schedule` ensures that layers are pruned according to a defined sparsity pattern.\n",
    "* Layers are organized sequantially, with ecah layer's output serving as the input to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code block below refers to a **Second Dense Layer**. It adds another dense layer with optional quantization and pruning. It is similar to the previous dense layer, but with 16 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "    x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adds batch normalization and activation layers with optional quantization and pruning. It is the same code block as the previous one about `QBatchNormalization` and `Activation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Latent Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mu = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)))(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)\n",
    "    logvar = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "                kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "                bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                    pruning_schedule=pruning_schedule)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `mu`: Mean of the latent space distribution.\n",
    "* `logvar`: Log variance of the latent space distribution.\n",
    "* Both are optionally quantized and pruned.\n",
    "* The code block above creates two dense layers for the latent space representation: one for the mean (`mu`) and one for the log variance (`logvar`). It Uses quantized dense layers (`QDense`) if `quanti_size` is not `0`, with specific quantizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    z = Sampling()([mu, logvar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `z`: Latent variable sampled using the reparameterization trick.\n",
    "* The `Sampling` layer takes `mu` and `logvar` as input and outputs a sampled latent vector `z`.\n",
    "* Note that in a VAE, we aim to encode input data into a latent space, and then decode it back to reconstruct the input. The VAE introduces stochasticity (randomness) in the encoding process, allowing it to learn a smooth latent space.\n",
    "* `mu` (the mean vector of the Gaussian distribution in the latent space) and `logvar` (The logarithm of the variance vector of the Gaussian distribution in the latent space) are outputs of the encoder netowrk. They represent the parameters of a Gaussian distribution. \n",
    "* The `Sampling` layer is a custom layer that uses the reparameterization trick to sample points from the Gaussian distribution defined by `mu` and `logvar`.\n",
    "* Geometrically, `mu` represents the center of the Gaussian distribution in the latent space and `logvar` determines the spread or dispersion around `mu`.\n",
    "* Directly sampling from a Gaussian distribution in the latent space could lead to problems in gradient computation during backpropagation. To address this, the **Reparameterization Trick** is used, which introduces a deterministic component (`mu`) and a stochastic component (sampled from a standard normal distribution).\n",
    "* `z` represents a point in the latent space that follows a Gaussian distribution. It encodes the information of the input data in a compact form. The stochastic nature of `z` allows the VAE to generate diverse samples from the learned distribution, enhancing its generative capabilities.\n",
    "* **Geometrically**, the encoder maps input data to a point `mu` in the latent space, with an associated spread determined by `logvar`. The `Sampling` layer then perturbs this point by adding a scaled random vector, resulting in `z`. This approach ensures that the latent space is populated in a way that captures the variability of the input data, allowing the decoder to reconstruct inputs from diverse latent codes.\n",
    "* `z = Sampling()([mu, logvar])` is a crucial step in the VAE pipeline that introduces controlled randomness into the latent space representation, facilitating effective training and meaningful data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')    \n",
    "    encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defines and summarizes the encoder model. The model was defined with `InputArray` as input and `[mu, logvar, z]` as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decoder Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `d_input`: Input layer for the decoder. It takes in a tensor of shape `latent_dim`. This tensor is the latent vector `z` sampled from the encoder.\n",
    "* Several dense layers with batch normalization and activation.\n",
    "* **The first dense layer** (`Dense(16)`) creates a dense (fully connected) layer with 16 units. The number of units is a hyperparameter that can be tuned. Then, `kernel_initializer=...` sets the weights of the dense layer according to the `HeNormal` initialization method, suitable for layers with `ReLU` activation functions. The `seed=42` ensures reproducibility.\n",
    "* **The first Batch Normalization** normalizes the outputs of the previous dense layer, improving training stability and speed.\n",
    "* **The first activation layer** applies the `ReLU` activation function, introducing non-linearity to the model and allowing it to learn more complex patterns.\n",
    "* **The second dense layer** creates another dense layer, now with 32 units and use again the HeNormal initializer and the seed.\n",
    "* Then, `BatchNormalization` and `Activation` are applied again. \n",
    "* **The final dense layer** creates a layer with the same number of units as the original input shape. This layer maps the latent space representation back to the original input space dimensions. As before, `HeNormal` initializer sets the weights.\n",
    "* `dec`: Output layer of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defines and summarizes the decoder model.\n",
    "* `Model(d_input, dec, name='decoder')` defines the decoder model, specifying `d_input` as the input and `dec` as the output. The name of the model is set to `decoder`.\n",
    "* `decoder.summary()` prints a summary of the decoder model, including the layers, their output shapes, and the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Return Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Returns the encoder and decoder models.\n",
    "* In summary, it is good to know the **Geometric Interpetation** of how the decoder works.\n",
    "    * ***Input Layer*** (`d_input`):\n",
    "        * The decoder starts with the latent vector z (of shape `(latent_dim,)`), representing a point in the latent space. The latent vector `z` lives in a space with dimensions defined by `latent_dim`. The `decoder` will process the `z` vector through several layers to transform it back to the original input space, producing an output with shape `(input_shape,)`.\n",
    "        * The latent space is a `latent_dim`-dimensional. This is typically much lower-dimensional than the original input space. The purpose of this dimensionality reduction is to capture the most important features of the input data in a compact representation.\n",
    "    * ***Dense and Activation Layers***:\n",
    "        * The first dense layer maps the latent vector to a 16-dimensional space, followed by `ReLU` activation.\n",
    "        * This transformation can be seen as the decoder learning to expand and interpret the compact representation (latent vector) into a higher-dimensional space.\n",
    "        * The second dense layer further maps this 16-dimensional representation to a 32-dimensional space, again followed by `ReLU` activation.\n",
    "    * ***Final Dense Layer***:\n",
    "        * The final dense layer maps the 32-dimensional representation back to the original input shape (the original input space is `input_shape`-dimensional, it is the space where the input data resides), effectively reconstructing the input data from the latent representation.\n",
    "* **What is the importance of the Decoder?**\n",
    "\n",
    "The decoder in this VAE setup takes the latent representation, gradually transforms it through several dense and activation layers, and finally reconstructs the input data. This process is essential for both learning a meaningful latent space and for generating new data samples.\n",
    "\n",
    "    * ***Reconstruction***\n",
    "        * The primary role of the decoder is to take the latent representation `z` and reconstruct it back to the original input space.\n",
    "    * ***Generative Capability***\n",
    "        * Because the decoder can reconstruct data from any point in the latent space, it can be used to generate new data by sampling from the latent space.\n",
    "    * ***Smoothness an Continuity***\n",
    "        * The structure of the decoder ensures that small changes in the latent space correspond to small changes in the reconstructed output, making the latent space smooth and continuous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
