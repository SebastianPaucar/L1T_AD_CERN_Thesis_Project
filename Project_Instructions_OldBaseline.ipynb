{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup environment\n",
    "We need to install miniconda (latest version). For this, we should put this in the terminal:\n",
    "\n",
    "`wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh`\n",
    "\n",
    "And then:\n",
    "\n",
    "`bash Miniconda3-py39_4.10.3-Linux-x86_64.sh`\n",
    "\n",
    "If close and open the terminal is necessary, we will have to put then:\n",
    "\n",
    "`source ~/.bashrc`\n",
    "\n",
    "The next step is to clone the repository:\n",
    "\n",
    "`git clone https://gitlab.cern.ch/l1a/l1_anomaly_ae.git`\n",
    "\n",
    "Then, we have to located at:\n",
    "\n",
    "`cd l1_anomaly_ae`\n",
    "\n",
    "However, the Jennifer's job is at the OldBaseline tag of the AD GitLab repo:\n",
    "\n",
    "`git checkout OldBaseline_KLL_MLT22`\n",
    "\n",
    "After that, we have to set up the conda environment:\n",
    "\n",
    "`conda env create -f l1ad.yml`\n",
    "\n",
    "Finally, it must be activated:\n",
    "\n",
    "`conda activate l1ad`\n",
    "## 2. Train and evaluate DNN models\n",
    "\n",
    "To run the full workflow, we have to execute in the terminal the following script:\n",
    "\n",
    "`python end2end.py --run all --config config.yml`\n",
    "\n",
    "* It runs the training and performance evaluation workflow.\n",
    "* It takes as input the h5 files with L1 AD data (see the GitLab).\n",
    "* Files location and model options are specified in `config.yml`.\n",
    "* The script saves the trained model and the results from the evaluation step in the output directory specified in `config.yml`.\n",
    "* To run using QKeras, we need to add `--model_quantize` in the script.\n",
    "* `--run` can be set to `--train` or `--eval` if we want to run only one step.\n",
    "* The training step dumps the data into a pickle file with name specified in `config.yml`, and it is saved in the same directory from which the script is run. We can skip this step with `--load_pickle`.\n",
    "* The evaluation step dumps the predictions from the training into `results.h5` in the output directory specified at `config.yml`. We can skip this step with `--load_results`\n",
    "## 3. Train and evaluate CNN models\n",
    "* ***Prepare the data***\n",
    "\n",
    "    `python prepare_data.py --input-file QCD_preprocessed.h5 --input-bsm BSM_preprocessed.h5 --output-file data.pickle`\n",
    "\n",
    "    * `prepares_data.py` prepares the input data. It accepts preprocessed QCD and BSM h5 files as input, and outputs a pickle file with train/test events (split) to be used for the train and evaluation steps.\n",
    "* ***Training***\n",
    "\n",
    "    `python train.py --latent-dim 4 --output-model-h5 output_model.h5 --output-model-json output_model.json --input-data data.pickle --output-history history.h5 --batch-size 256 --n-epochs 30`\n",
    "\n",
    "    * It requires:\n",
    "        * Input of the latent dimension.\n",
    "        * Filenames for the output model (h5 and json).\n",
    "        * The training data pickle file (see *Prepare the data*).\n",
    "        * The history, the batch size and the nomber of epochs.\n",
    "    * Two model can be trained: Conv VAE or Conv AE. The model can be set with `--model type` as either `conv_vae` or `conv_ae`.\n",
    "* ***Performance evaluation***\n",
    "    * Prepare file with predictions for the QCD test sample and BSM samples:\n",
    "\n",
    "        `python evaluate.py --input-h5 output_model.h5 --input-json output_model.json --input-history history.h5 --output-result result.h5 --input-file data.pickle`\n",
    "\n",
    "    * make ROC curves, history and loss distributions:\n",
    "\n",
    "        `python plot.py --coord cyl --model vae --loss-type mse_kl --output-dir ./ --input-dir ./ --label my_vae`\n",
    "\n",
    "        Several loss types can be set and type of features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
