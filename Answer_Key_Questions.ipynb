{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `h5py`: For handling HDF5 files.\n",
    "* `argparse`: \n",
    "* `tensorflow`:\n",
    "* `tensorflow.keras`:\n",
    "    * `backend` (K): \n",
    "* `tensorflow.kers.models import Model`:\n",
    "* `tensorflow.keras.layers`:\n",
    "    * `Input`:\n",
    "    * `Dense`:\n",
    "    * `Lambda`:\n",
    "    * `BatchNormalization`:\n",
    "    * `Activation`:\n",
    "    * `Concatenate`:\n",
    "    * `Dropout`:\n",
    "    * `Layer`: \n",
    "    * `ReLU`:\n",
    "    * `LeakyReLU`:\n",
    "* **Custom Imports:**\n",
    "    * `from autoencoder_classes import AE`:\n",
    "    * `from custom_layers import Sampling`:\n",
    "* `tensorflow.keras.callbacks`:\n",
    "    * `EarlyStopping`:\n",
    "    * `ReduceLROnPlateau`:\n",
    "    * `TerminateOnNaN`:\n",
    "* `neptunecontrib.monitoring.keras`:\n",
    "    * `NeptuneMonitor`: \n",
    "* `qkeras`:\n",
    "    * `QDense`:\n",
    "    * `QActivation`:\n",
    "    * `QBatchNormalization`:\n",
    "* `import tensorflow_model_optimization as tfmot`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What about *Bandwith Limitations* at CERN LHC?**\n",
    "\n",
    "The LHC generates an immense volume of data from the collision events, with potentially hundreds of terabytes of data per second. However, the infrastructure for data ***transmission, storage, and real-time processing*** has practical limits, often defined by the maximum rate at which data can be transferred and analyzed.\n",
    "\n",
    "Due to these limitations, the LHC's data acquisition system cannot handle all the data produced by the detectors. Therefore, ***most of the collision events must be quickly assessed and filtered by online selection systems***, such as the ***Level-1 Trigger***. These systems are designed to discard the majority of events that are unlikely to be of scientific interest, ***allowing only a manageable amount of data*** (those events most likely to yield significant discoveries) to be saved for detailed analysis. This process helps to cope with the bandwidth limitations and ensures that the essential data can be stored and studied without overwhelming the available resources.\n",
    "\n",
    "# **What about the first selection stage (L1T)? Why does it discard events (reducing the event rate) within a few micro-time?**\n",
    "\n",
    "The L1T system has a extremely short time frame to operating. Specifically, it means that the L1T system must analyze and make decisions about whether to keep or discard collision events extremely quickly, on the order of a few millionths of a second (microseconds).\n",
    "\n",
    "The ***L1T*** system ***uses*** custom electronic boards equipped with field-programmable gate arrays (***FPGAs***) ***to execute its algorithms***. ***These algorithms*** rapidly ***assess each event based on predefined criteria*** to determine if it should be retained ***for further processing by*** the High Level Trigger (***HLT***) system. The \"few microseconds\" limit is a crucial aspect because the LHC generates a vast number of events per second, and ***the system must filter them at high speed to reduce the data rate*** effectively, ***ensuring only potentially interesting events are passed to the next stage for more detailed analysis***.\n",
    "\n",
    "# **What about *bias* in the context of DAQ (Selection & Triggering) systems at the L1-LHC? What is its potential for using *online* unsupervised learning methods?**\n",
    "\n",
    "The L1T system is currently biased due to its reliance on predefined selection criteria, primarily based on energy thresholds. This bias can exclude potentially interesting events that do not meet these criteria. By implementing unbiased selection algorithms at the L1T stage, the system could collect data without preconceived filters, thus maintaining a wider spectrum of events for analysis and increasing the chances of discovering new physics phenomena.\n",
    "\n",
    "* **Bias in Data Selection**:\n",
    "    * In particle physics experiments, a bias in data selection occurs when ***certain criteria or thresholds are applied to decide which events are worth keeping for further analysis***. This means that the system preferentially selects events that meet specific conditions, potentially ***excluding other events that do not fit these criteria***. For instance, in the case of the ***L1T, the current selection algorithms are biased*** towards events with a minimum energy threshold, meaning only events where the detected particles have energies above a certain level are kept. This is referred to as a ***selection bias*** because ***it predisposes the data collected towards certain types of events, potentially ignoring others***.\n",
    "* **Unbiased Selection**:\n",
    "    * An ***unbiased*** approach refers to a ***method of selecting events that does not rely on specific predefined criteria like energy thresholds***. Instead, an unbiased selection would involve using algorithms that ***identify events based*** on their ***degree of abnormality*** or other ***signal-independent characteristics***. This approach aims to ***avoid preconceived notions about what constitutes an interesting event***, thus ***providing*** a more comprehensive and ***diverse dataset for analysis***. The idea is to ***allow for the discovery of unexpected phenomena that might not fit within the standard criteria*** used in a biased selection process.\n",
    "* **L1T and Bias**\n",
    "    * **Current Situation with L1T**:\n",
    "        * The **L1T** system **is** described as being **biased** because it employs relatively simple, theory-motivated selection algorithms. These algorithms are designed to quickly reduce the massive data rate by discarding events that are less likely to be of interest based on known physics models. This process typically involves setting thresholds, such as a minimum energy requirement, that events must meet to be retained. As a result, lower-energy events, which might be less likely to reveal new or interesting physics according to current theories, are excluded from further analysis\n",
    "    * **Potential for Unbiased Selection at L1T**:\n",
    "        * The ***online unsupervised learning could be more effectively used in the L1T stage to reduce or eliminate this bias***. If deployed at the L1T level, such techniques ***could identify unusual or abnormal events without relying on strict, theory-based criteria like energy thresholds. This approach could lead to a more comprehensive collection of data, including events that might be overlooked by current methods due to their failure to meet the existing biased criteria.\n",
    "    * **Transition from L1T to HLT**:\n",
    "        * The ***HLT operates after the L1T and can apply more complex algorithms***, benefiting from more computing resources and time. While the ***HLT can refine and further reduce the event data passed on from the L1T***, the initial selection ***bias introduced by the L1T*** means that some potential ***discoveries could already be lost*** by the time data reaches the HLT. Therefore, introducing ***unbiased selection*** earlier in the pipeline, ***at the L1T stage***, could ***allow*** for a more ***diverse and potentially more informative dataset***, providing ***opportunities to discover new physics*** beyond current expectations.\n",
    "\n",
    "# More details about the L1T: What about the extremely low latency required? What is the *Initation Intrval* and why is it at the nanosecond order? How is it related to the bunch-crossing time for the LHC upcoming period? What is the upcoming LHC period?\n",
    "\n",
    "The ***L1T system's design*** revolves around ***minimizing latency, maximizing processing speed, and efficiently using computational resources***. The use of ***FPGAs***, along ***with*** innovative software libraries like ***hls4ml***, plays a crucial role in achieving these goals, ***ensuring*** that the ***LHC experiments can continue to explore*** the frontiers of particle physics.\n",
    "\n",
    "The requirement for ***low latency*** is not only a technical challenge but also ***a necessity to capture the rare and potentially groundbreaking physics events*** that occur in the LHC. Missing or delaying these events could mean losing critical data needed for new discoveries.\n",
    "\n",
    "As the **LHC** continues to evolve with ***upgrades like the HL-LHC***, the ***data rate and complexity will increase***. The trigger systems, including ***L1T***, will ***need to scale*** accordingly, ***incorporating more advanced algorithms and processing techniques*** while ***maintaining or even reducing latency***.\n",
    "\n",
    "* **Extremely Low Latency Requirement for L1T:**\n",
    "\n",
    "    The Level-1 Trigger (***L1T***) in the LHC experiments must ***operate with extremely low latency***, typically around ***1 microsecond*** (μs). This stringent requirement is due to several factors:\n",
    "    * **High Event Rate**: The LHC generates proton-proton collisions at an extremely high rate, approximately 40 million collisions per second. This high frequency of events produces a massive amount of data that needs to be processed almost instantaneously to determine which events are worth saving for further analysis.\n",
    "    * **Data Volume Management**: The detectors produce raw data at rates of up to hundreds of terabytes per second. The L1T must quickly reduce this data volume by filtering out uninteresting events to prevent overwhelming the subsequent data processing and storage systems.\n",
    "    * **Bunch-Crossing Time**: At the LHC, ***bunches of protons collide every 25 nanoseconds (ns)***. This interval, known as the bunch-crossing time, sets a ***natural time frame for processing***. The ***L1T has to make a decision within a very short period (around 1 μs)*** to ensure it can keep up with the continuous stream of incoming data from these rapid collisions.\n",
    "\n",
    "* **Role of FPGAs in L1T:**\n",
    "\n",
    "    Field-Programmable Gate Arrays (FPGAs) are crucial in implementing the L1T system due to their:\n",
    "    * **High-Speed Processing**: FPGAs can execute ***parallel operations with very low latency***, making them ***ideal for the high-speed decision-making*** required ***in L1T***. They are programmed to perform specific algorithms that analyze event data rapidly.\n",
    "    * **Flexibility**: Unlike application-specific integrated circuits (ASICs), ***FPGAs can be reprogrammed, allowing for updates*** and modifications ***to the trigger algorithms without needing new hardware***. This flexibility is vital for adapting to new physics discoveries or changes in experimental conditions.\n",
    "    * **Resource Efficiency**: ***FPGAs*** can efficiently ***manage computational tasks with limited resources***, which is ***essential*** given the need ***to implement multiple algorithms on the same hardware platform***.\n",
    "    \n",
    "    The ***hls4ml*** library facilitates the ***translation of machine learning models***, like neural networks and boosted decision trees, ***into FPGA firmware***. This allows complex algorithms to be implemented on the FPGAs while ***respecting the strict latency and resource constraints of the L1T system***.\n",
    "\n",
    "* **Initiation Interval (II) and Bunch-Crossing Time:**\n",
    "\n",
    "    * The Initiation Interval (II) refers to the ***minimum time interval between the start of consecutive executions of an algorithm***. For the ***L1T***, this interval ***must be extremely short***, around ***150 ns***, to ***match the LHC's bunch-crossing time of 25 ns***. Although the ***L1T has up to 1 μs to make a decision, the algorithms must be designed to operate within shorter intervals*** to process each event efficiently and allow for overlap in the processing of multiple events.\n",
    "\n",
    "* **Upcoming Period of LHC Operations:**\n",
    "\n",
    "    * The \"upcoming period of the LHC operations\" likely refers to the next phase of the LHC's operational schedule, such as Run 3, which started in 2022, or the future ***High-Luminosity LHC (HL-LHC) upgrade***. The HL-LHC ***aims to increase the number of collisions and data precision***, posing additional challenges for data processing and ***requiring even more efficient and sophisticated trigger systems***. This upcoming period will demand ***improved trigger algorithms to handle the higher data rates and complexities associated with higher luminosity***.\n",
    "\n",
    "* **Quantization-Aware Training (QAT) and hls4ml:**\n",
    "\n",
    "    * **Quantization-Aware Training (QAT):** This technique involves ***training machine learning models with considerations for quantization***, which ***reduces the number of bits used to represent model parameters and operations***. QAT helps in ***compressing models without significant loss in accuracy***, making them more efficient for FPGA implementation.\n",
    "    * **hls4ml:** This open-source library is designed to facilitate the conversion of machine learning models into FPGA-compatible firmware. ***By supporting QAT*** and other optimizations, ***hls4ml enables the deployment of complex anomaly detection algorithms on FPGAs within the tight resource and latency constraints of the L1T system***."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
