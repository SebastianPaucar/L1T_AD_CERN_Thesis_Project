{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OldBaseline_KLL_MLT22 Git revision: End2End_demo.ipynb (EXPLANATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy, save_model , make_mse_loss, mse_cart_loss, mse_cart_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "from model import build_AE, build_VAE, Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the next libraries and dependencies that are being imported:\n",
    "* `h5py`: For reading and writing **HDF5** files.\n",
    "* `sklearn.model_selection`: For splitting the dataset into training and test sets.\n",
    "* `joblib`: For saving and loading Python objects efficiently.\n",
    "* `pickle`: For **serializing and de-serializing Python objects**.\n",
    "* `sklearn.preprocessing`: For scaling and transforming data.\n",
    "* `sys`: For system-specific parameters and functions.\n",
    "* `gc`: For garbage collection.\n",
    "* `tensorflow.keras as keras`: For high-level API to build and train models.\n",
    "* `Model, Input, Dense, Lambda, ...`: Core components and layers from Keras for building neural networks.\n",
    "* `K`: Keras backend for lower-level operations.\n",
    "* `tf.keras.mixed_precision.set_global_policy('mixed_float16')`: For setting **mixed precision policy** to improve performance.\n",
    "* `tensorboard`: For visualizing the training process.\n",
    "* `os, pathlib`: For file and directory operations.\n",
    "* `mplhep`: For high-energy physics plotting style (ROOT style).\n",
    "* `EarlyStopping, ReduceLROnPlateau, TerminateOnNaN`: Keras **callbacks** for controlling the training process.\n",
    "* `def return_total_loss(loss, bsm_t, bsm_pred)`: A utility function that computes the total loss given a loss function, true labels `bsm_t`, and predicted labels `bsm_pred`.\n",
    "* ***Custom modules***\n",
    "    * `autoencoder_classes`: Custom module containing the definitions for AE and VAE classes.\n",
    "    * `NeptuneMonitor`: Custom callback for integrating with Neptune for experiment tracking.\n",
    "    * `losses`: Custom module containing loss functions.\n",
    "    * `functions`: Custom module containing utility functions.\n",
    "    * `sklearn.metrics`: For computing ROC curves and AUC metrics.\n",
    "    * `data_preprocessing`: Custom module for preparing the data.\n",
    "    * `model`: Custom module for building AE, VAE models, and the Sampling layer.\n",
    "\n",
    "<span style=\"color: red;\"> Key task: Research about all new packages and dependencies that were called. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-ZB-h5-extended-v2/ZB_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2-120X/BSM_preprocessed.h5\"\n",
    "events = 2000000\n",
    "load_pickle=True\n",
    "train=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "latent_dim=3\n",
    "batch_size= 1024\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration block sets up various parameters and file paths that will be used later in the demo notebook.\n",
    "* ***File Paths and Data***\n",
    "    * `input_qcd`: Path to the processed QCD dataset stored in HDF5 format. This file likely contains QCD events that will be used either for training, validation, or testing the model.\n",
    "    * `input_bsm`: Path to the preprocessed BSM dataset stored in HDF5 format. This file contains BSM events for similar purposes as the QCD file.\n",
    "    * These files are likely located on a distributed system used in HEP experiments, specifically on the EOS large-scale storage system solution used at CERN. `/eos/uscms/` suggests they are part of the US CMS storage at CERN. At lxplus, you can access to the EOS environment through `xrdfs eospublic.cern.ch`.\n",
    "    \n",
    "     <span style=\"color: red;\"> Key task: Research about eospublic, how can I access to jngadiub files? </span>\n",
    "* ***Data Handling***\n",
    "    * `events` specifies that 2 millions events will be loaded from the dataset. It is the number of events to be used.\n",
    "    * `load_pickle`: A boolean flag to indicate whether to load preprocessed data from a pickle file. If `True`, the notebook will load data from a pickle file instead of preprocessing it from the raw HDF5 files.\n",
    "    * `train`: A boolean flag indicating whether to train the model. If `False`, the notebook might skip the training step and load a pre-trained model instead.\n",
    "* ***File Paths for Intermediate and Output Data***\n",
    "    * `input_pickle`: Path to the pickle file containing preprocessed data. This file will be loaded if `load_pickle` is `True`.\n",
    "    * `output_pfile`: Path to save the preprocessed data in pickle format. This file will be created or overwritten to save the preprocessed data.\n",
    "    * `output_model_h5`: Path to save the trained model in HDF5 format. The trained model will be saved here if training is performed.\n",
    "    * `output_model_json`: Path to save the model architecture in JSON format. The model architecture will be saved in a JSON file for potential reconstruction later.\n",
    "    * `output_history`: Path to save the training history. The training history, which includes loss and metrics over epochs, will be saved in this HDF5 file.\n",
    "    * `output_result`: Path to save the results of the model evaluation.The evaluation results, such as predictions or performance metrics, will be saved in this HDF5 file.\n",
    "* ***Model Configuration***\n",
    "    * `model_type`: The type of model to be used, either `AE` for Autoencoder or `VAE` for Variational Autoencoder.\n",
    "    * `latent_dim`: The dimensionality of the latent space.\n",
    "* ***Training Parameters***\n",
    "    * `batch_size`: The number of samples per gradient update. The model will process that amount of samples at a time during training.\n",
    "    * `n_epochs`: The number of epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(load_pickle):\n",
    "    if(input_pickle==''):\n",
    "        print('Please provide input pickle files')\n",
    "    with open(input_pickle, 'rb') as f:\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "        bsm_types = [ 'GluGluHToTauTau_M125',\n",
    "\t\t      'GluGluToHHTo4B_cHHH1',\n",
    "\t\t      'GluGluToHHTo4B_cHHH5',\n",
    "\t\t      'HTo2LongLivedTo4b_MH1000_MFF450_CTau100m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH1000_MFF450_CTau10m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF12_CTau9m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF12_CTau0p9m', \n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF25_CTau15m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF25_CTau1p5m', \n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF50_CTau30m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH125_MFF50_CTau3m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH250_MFF120_CTau10m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH250_MFF120_CTau1m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH250_MFF60_CTau1m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF160_CTau10m', \n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF160_CTau1m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF160_CTau0p5m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF80_CTau10m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF80_CTau1m',\n",
    "\t\t      'HTo2LongLivedTo4b_MH350_MFF80_CTau0p5m',\n",
    "\t\t      'HTo2LongLivedTo4mu_MH1000_MFF450_CTau10m',\n",
    "\t\t      'HTo2LongLivedTo4mu_MH125_MFF12_CTau0p9m',\n",
    "\t\t      'HTo2LongLivedTo4mu_MH125_MFF25_CTau1p5m',\n",
    "\t\t      'HTo2LongLivedTo4mu_MH125_MFF50_CTau3m',\n",
    "\t\t      'SUSYGluGluToBBHToBB_M1200',\n",
    "\t\t      'SUSYGluGluToBBHToBB_M120',\n",
    "\t\t      'SUSYGluGluToBBHToBB_M350',\n",
    "\t\t      'SUSYGluGluToBBHToBB_M600',\n",
    "\t\t      'TprimeBToTH_M650',\n",
    "\t\t      'VBFHHTo4B_CV_1_C2V_2_C3_1',\n",
    "\t\t      'VBFHToInvisible_M125',\n",
    "\t\t      'VBFHToTauTau_M125',\n",
    "\t\t      'VectorZPrimeGammaToQQGamma_M10_GPt75',\n",
    "\t\t      'VectorZPrimeToQQ_M100_Pt300',\n",
    "\t\t      'VectorZPrimeToQQ_M200_Pt300',\n",
    "\t\t      'haa4b_ma60',\n",
    "\t\t      'haa4b_ma15',\n",
    "\t\t      'haa4b_ma15_powheg',\n",
    "\t\t      'TT'\n",
    "                    ]\n",
    "else:\n",
    "    if(input_qcd==''or input_bsm==''):\n",
    "        print('Please provide input H5 files')\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, 'Cartesian', output_pfile,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block shows how the script either loads preprocessed data from a pickle file or reads and preprocesses raw data from H5 files, depending on the `load_pickle` flag. This allows for flexibility in the data loading process, making it easier to switch between using preprocessed data and raw data.\n",
    "\n",
    "* In principle, the code above checks `load_pickle`:\n",
    "    * If `load_pickle` is `True`:\n",
    "        * Check if `input_pickle` if empty. If it is, the program prints a message asking to provide input pickle files.\n",
    "        * If it is not empty, inside `with open(input_pickle, 'rb') as f` the pickle file in binary read mode (`rb`) will be opened. `pickle.load(f)` reads the data from the file and deserializes into the listed variables.\n",
    "    * If `load_pickle` is `False`:\n",
    "        * Check if either `input_qcd` or `input_bsm` is empty. If either of the inputs is an empty string, it prints a message asking to provide input H5 files.\n",
    "        * Once you provide the input files, the customized function `prepare_data()` will read and preprocesses them, and returns several variables as `X_train_flatten, X_train_scaled, ...`.\n",
    "* It is good to know that:\n",
    "    * **Pickle File**: A pickle file is used to store Python objects in a serialized format. This is useful for saving preprocessed data and loading it later without having to preprocess it again.\n",
    "    * **H5 file**: An H5 file is a hierarchical data format file used to store large amounts of data.\n",
    "    * **Data Preprocessing**: The `prepare_data` function handles the conversion and preprocessing of raw data from H5 files into a format suitable for training and testing machine learning models. This includes tasks like flattening multidimensional arrays and scaling data to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1],latent_dim)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1],latent_dim)\n",
    "        model = VAE(encoder, decoder, mse_cart_loss)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    print(\"Training the model\")\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The initial condition checks if the `train` flag is set to `True`. If so, the model training process will be initiated. \n",
    "    * ***Building the Autoencoder***:\n",
    "        * `build_AE(X_train_flatten.shape[-1], latent_dim)` creates an AE model. The input size is determined by the shape of `X_train_flatten` and the latent dimension is specified by `latent_dim`.\n",
    "        * `model = AE(autoencoder)` wraps the autoencoder in a class (presumably defined elsewhere, `AE()` is a function defined in a customized library that was called in the import statement code block using `autoencoder_classes`) that includes additional methods or properties specific to the project.\n",
    "        * `model.compile(optimizer=keras.optimizers.Adam(lr=0.001))` compiles the model using the Adam optimizer with a learning rate of 0.001.\n",
    "        * ***Setting up Callbacks***\n",
    "            * `ReduceLROnPlateau`: Reduces the learning rate by a factor of 0.1 if the validation loss doesn't improve for 2 epochs, with a minimum delta of 0.0001, a cooldown period of 2 epochs, and a minimum learning rate of 1e-6.\n",
    "            * `TerminateOnNaN()`: Stops training if any NaN values are encountered.\n",
    "            * `NeptuneMonitor()`: Tracks the training process using `Neptune` for monitoring and logging.\n",
    "            * `EarlyStopping`: Stops training if the validation loss doesn't improve for 10 epochs and restores the best weights observed during training.\n",
    "    * ***Building the Variational Autoencoder***\n",
    "        * `build_VAE(X_train_flatten.shape[-1], latent_dim)`: Creates the encoder and decoder parts of a VAE model.\n",
    "        * `model = VAE(encoder, decoder, mse_cart_loss)`: Initializes a VAE model with the encoder, decoder, and a custom loss function `mse_cart_loss`.\n",
    "        * `model.compile(optimizer=keras.optimizers.Adam())`: Compiles the model using the Adam optimizer with default settings.\n",
    "        * Callbacks were setting up in the same way as in the AE case.\n",
    "    * ***Training The model***\n",
    "        * `model.fit(...)`: Trains the model using the training data (`X_train_flatten and X_train_scaled`) for a specified number of epochs (`n_epochs`) and batch size (`batch_size`).\n",
    "            * `validation_split=0.2`: Reserves 20% of the training data for validation.\n",
    "            * `callbacks=callbacks`: Uses the callbacks defined earlier to optimize and manage the training process effectively.\n",
    "    * ***Cleanup***\n",
    "        * `del X_train_flatten, X_train_scaled`: Deletes the training data from memory to free up space.\n",
    "        * `gc.collect()`: Invokes the garbage collector to free up unreferenced memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "            print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        print(\"Saved history to disk\")\n",
    "\n",
    "\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    print(hist.tail())\n",
    "    plt.plot(hist.index.to_numpy(),hist['loss'],label='Loss')\n",
    "    plt.plot(hist.index.to_numpy(),hist['val_loss'],label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig('history.pdf')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `if(train)` checks if the `train` flag is set to `True`. If so, the conde inside this block will execute.\n",
    "    * `if(output_model_h5 != '')` chekcs if `output_model_h5` is specified, i.e, this condition checks if the `output_model_h5` string is not empty, indicating that a path to save the model is specified.\n",
    "        * ***Save the VAE Model***: If the model type is VAE, the model is saved using the TensorFlow/Keras save method. `os.path.join(os.getcwd(), output_model_h5.split('.')[0])` creates the path to save the model in the current working directory. The `split('.')[0]` part removes the file extension from `output_model_h5`.\n",
    "        * ***Save the AE Model***: If the model type is AE, the autoencoder's architecture is saved as a JSON file. `autoencoder.to_json()` converts the model architecture to a JSON string.\n",
    "            * `with open(output_model_json, 'w') as json_file` opens the file specified by `output_model_json` in write mode. `json_file.write(model_json)` then writes the JSON string to the file.\n",
    "            * `autoencoder.save_weights(output_model_h5)` saves the weights of the autoencoder model to the file specified by `output_model_h5`.\n",
    "    * ***Saving the Training History***\n",
    "        * `if(output_history != '')` checks if `output_history` is specified, i.e, it checks if the `output_history` string is not empty, indicating that a path to save the training history is specified.\n",
    "        * `with open(output_history, 'wb') as f` opens the file specified by `output_history` in write-binary mode.\n",
    "        * `pickle.dump(history.history, f)` serializes the training history dictionary and writes it to the file.\n",
    "    * ***Plotting the Training and Validation Loss***\n",
    "        * `hist = pd.DataFrame(history.history)` Converts the training history dictionary to a pandas DataFrame.\n",
    "        * `hist['epoch'] = history.epoch`: Adds an `epoch` column to the DataFrame, containing the epoch numbers.\n",
    "        * `print(hist.tail())`: Prints the last few rows of the DataFrame to the console.\n",
    "        * `plt.plot(hist.index.to_numpy(), hist['loss'], label='Loss')`: Plots the training loss over epochs.\n",
    "        * `plt.plot(hist.index.to_numpy(), hist['val_loss'], label='Val Loss')`: Plots the validation loss over epochs.\n",
    "        * `plt.yscale('log')`: Sets the y-axis scale to logarithmic.\n",
    "        * `plt.legend()`: Adds a legend to the plot to differentiate between the training and validation loss.\n",
    "        * `plt.savefig('history.pdf')`: Saves the plot as a PDF file named `history.pdf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model_dir = output_model_h5.split('.')[0]\n",
    "if(model_type=='AE'):\n",
    "    with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "    ae = tf.keras.models.model_from_json(config)    \n",
    "    ae.load_weights(model_dir+\"/model.h5\")\n",
    "    ae.summary()\n",
    "    model = AE(ae)\n",
    "elif(model_type=='VAE'):\n",
    "    encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "    encoder.summary()\n",
    "    decoder.summary()\n",
    "    model = VAE(encoder, decoder, mse_cart_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above is responsible for loading a previously saved model.\n",
    "* ***Define the Model Directory***\n",
    "    * `model_dir = output_model_h5.split('.')[0]`: This line takes the `output_model_h5` string, splits it at the period `.` character, and takes the first part. This effectively removes the file extension, leaving just the directory or base filename. For example, if `output_model_h5` is `model.h5`, `model_dir` will be `model`.\n",
    "* ***Check Model Type***\n",
    "    * `if(model_type=='AE')` checks if the `model_type` is set to `AE`.\n",
    "        * `with open(model_dir+\"/model.json\", 'r') as jsonfile` opens the JSON file containing the model architecture in read mode.\n",
    "        * `config = jsonfile.read()` reads the entire file content into the `config` variable.\n",
    "        * `ae = tf.keras.models.model_from_json(config)` uses the JSON configuration to recreate the model architecture.\n",
    "        * `ae.load_weights(model_dir+\"/model.h5\")` loads the weights into the model from the specified H5 file.\n",
    "        * `ae.summary()` prints a summary of the model architecture to the console.\n",
    "        * `model = AE(ae)` wraps the loaded model into an `AE` object.\n",
    "    * `elif(model_type=='VAE')` checks if the model_type is set to `VAE` (Variational Autoencoder).\n",
    "        * `encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})` uses the custom `Sampling` layer to load the encoder and decoder models from the specified directory.\n",
    "        * `model = VAE(encoder, decoder, mse_cart_loss)` creates a `VAE` object using the loaded encoder and decoder models, and specifies the loss function `mse_cart_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from end2end import get_results\n",
    "data_file = input_pickle\n",
    "outdir = output_model_h5.split('.')[0]\n",
    "if not load_pickle: data_file = output_pfile\n",
    "results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim,'Cartesian')\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `get_results`, imported from `end2end` is responsible for loading a trained model and evaluating it on test data to produce results and return them in a dictionary format.\n",
    "* `data_file` is set to the value of `input_pickle` (path to the pickle file that contains preprocessed data), which will be used to load the data for evaluation. \n",
    "* `outdir` is set to the name of the directory where the model output will be stored(basically it is `model_dir`).\n",
    "* If `load_pickle` is `False` (i.e, there is not a input pickle file with preprocessed data), `data_file` will be set to `output_pfile` (file created to save the preprocessed data in pickle format) instead of `input_pickle`.\n",
    "* `get_results(...,'Cartesian')` refers to the normalization strategy to be used. \n",
    "* `print(results.keys())` gives an overview of the different datasets or metrics that were evaluated and stored in the results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only a few signals to check performance\n",
    "results_small = {}\n",
    "keys = ['QCD','TT','haa4b_ma15_powheg','GluGluToHHTo4B_cHHH1','GluGluHToTauTau_M125']\n",
    "for k in keys:\n",
    "    results_small[k] = results[k]\n",
    "results = results_small\n",
    "del results_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above selects a subset of signales from the full set of results and stores them in a new dictionary.\n",
    "* `results_small` is initialized as an empty dictionary. This will store only a subset of the original results.\n",
    "* `keys` is a list containing the names of the signals that you want to keep in `results_small`. The code within to `for k in keys` filters the `results dictionary` to only include the entries specified in `keys`. Then, `results` is reasigned to reference `results_small`\n",
    "* `del results_small` deletes the temporary `results_small` to free up memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss,max_loss=1e5,0\n",
    "if(model_type=='VAE'):\n",
    "    min_tloss,max_tloss=1e5,0\n",
    "    min_r,max_r=1e5,0\n",
    "for key in results.keys():\n",
    "    # if(key=='QCD'): continue\n",
    "    results[key]['loss'][results[key]['loss'][:] == np.inf] = 0\n",
    "    results[key]['total_loss'][results[key]['total_loss'][:] == np.inf] = 0\n",
    "    results[key]['radius'][results[key]['radius'][:] == np.inf] = 0\n",
    "    results[key]['kl_loss'][results[key]['kl_loss'][:] == np.inf] = 0\n",
    "    if(np.min(results[key]['loss'])<min_loss): min_loss = np.min(results[key]['loss'])\n",
    "    if(np.max(results[key]['loss'])>max_loss): max_loss = np.max(results[key]['loss'])\n",
    "    if(model_type=='VAE'):\n",
    "        if(np.min(results[key]['total_loss'])<min_tloss): min_tloss = np.min(results[key]['total_loss'])\n",
    "        if(np.max(results[key]['total_loss'])>max_tloss): max_tloss = np.max(results[key]['total_loss'])\n",
    "        # if(max_tloss>np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss'])): max_tloss = np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss'])\n",
    "        print(key,\"Total_loss\",np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss']))\n",
    "        if(np.min(results[key]['radius'])<min_r): min_r = np.min(results[key]['radius'])\n",
    "        if(np.max(results[key]['radius'])>max_r): max_r = np.max(results[key]['radius'])\n",
    "        # if(max_r>np.mean(results[key]['radius'])+10*np.std(results[key]['radius'])): max_r = np.mean(results[key]['radius'])+10*np.std(results[key]['radius'])\n",
    "        print(key,\"radius\",np.mean(results[key]['radius'])+10*np.std(results[key]['radius']))\n",
    "print(min_loss,max_loss)\n",
    "print(min_tloss,max_tloss)\n",
    "print(min_r,max_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag='vtest'\n",
    "print(\"Plotting the results\")\n",
    "bins_=np.linspace(min_loss,max_loss,100)\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if('TT' in key): key_ = 'TTbar'\n",
    "    if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "    if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "    if('TauTau' in key): key_ = 'SM H -> 2tau'    \n",
    "    if(key=='QCD'): plt.hist(results[key]['loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "    else: plt.hist(results[key]['loss'],label=key_,histtype='step',bins=bins_,density=True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('MSE Loss')\n",
    "plt.ylabel('Density')\n",
    "plt.title('MSE Loss distribution')\n",
    "plt.savefig('{outdir}/MSE_loss_{model_type}.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "if(model_type=='VAE'):\n",
    "\n",
    "    bins_=np.linspace(min_tloss,32673,100)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'        \n",
    "        if(key=='QCD'): plt.hist(results[key]['total_loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "        else: plt.hist(results[key]['total_loss'],label=key_,histtype='step',bins=bins_,density=True)\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Total Loss')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Total Loss distribution')\n",
    "    plt.savefig('{outdir}/Total_loss_{model_type}.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "\n",
    "    bins_=np.linspace(min_r,10,100)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'           \n",
    "        if(key=='QCD'): plt.hist(results[key]['radius'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "        else: plt.hist(results[key]['radius'],label=key_,histtype='step',bins=bins_,density=True)\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Radius')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Radius distribution')\n",
    "    plt.savefig('{outdir}/Radius_{model_type}.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "\n",
    "    bins_=np.linspace(min_r,10,100)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'           \n",
    "        if(key=='QCD'): plt.hist(results[key]['kl_loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "        else: plt.hist(results[key]['kl_loss'],label=key_,histtype='step',bins=bins_,density=True)\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('KL Loss')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('KL Loss distribution')\n",
    "    plt.savefig('{outdir}/KL_loss_{model_type}.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "    \n",
    "#     for key in results.keys():\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for i in range(latent_dim):\n",
    "#             plt.hist(results[key]['mean_prediction'][:,i],bins=100,label='mean '+str(i),histtype='step', density=True,range=[-5,5])\n",
    "#         plt.legend(fontsize='x-small')\n",
    "#         plt.xlabel('Loss')\n",
    "#         plt.ylabel('z')\n",
    "#         plt.title(key+' mean Z distribution')\n",
    "#         plt.savefig('mean_z_'+model_type+'_'+key+'_'+tag+'.pdf')\n",
    "#         # plt.show()\n",
    "\n",
    "#     for key in results.keys():\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for i in range(latent_dim):\n",
    "#             plt.hist(results[key]['logvar_prediction'][:,i],bins=100,label='logvar '+str(i),histtype='step', density=True,range=[-20,20])\n",
    "#         plt.legend(fontsize='x-small')\n",
    "#         plt.xlabel('Loss')\n",
    "#         plt.ylabel('z')\n",
    "#         plt.title(key+' logvar Z distribution')\n",
    "#         plt.savefig('logvar_z_'+model_type+'_'+key+'_'+tag+'.pdf')\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make reconstruction plots\n",
    "plt.figure(figsize=(10,10))\n",
    "bkg_key='QCD'\n",
    "bins_ = np.linspace(-50,50,100)\n",
    "results[bkg_key]['target'] = results[bkg_key]['target'].reshape((results[bkg_key]['target'].shape[0],19,3))\n",
    "test = results[bkg_key]['target'][:,1:9,:]\n",
    "test = test.reshape(results[bkg_key]['target'].shape[0]*8,3)\n",
    "mask0 = test[:,0]!=0\n",
    "mask1 = test[:,1]!=0\n",
    "mask2 = test[:,2]!=0\n",
    "mask = mask0 + mask1 + mask2\n",
    "test = test[mask]\n",
    "results[bkg_key]['prediction'] = results[bkg_key]['prediction'].reshape((results[bkg_key]['prediction'].shape[0],19,3))\n",
    "predict = results[bkg_key]['prediction'][:,1:9,:]\n",
    "predict = predict.reshape(results[bkg_key]['prediction'].shape[0]*8,3)\n",
    "predict = predict[mask]\n",
    "plt.hist(test.flatten(), bins=bins_, alpha=0.5, label='X_test')\n",
    "plt.hist(predict.flatten(), bins=bins_, alpha=0.5, label='X_predict')\n",
    "plt.xlabel('Pi (GeV)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yscale('log')\n",
    "plt.title('Muon, electon, and photon pt')\n",
    "plt.savefig( '{outdir}/reco_{model_type}_leptons.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "bkg_key='QCD'\n",
    "bins_ = np.linspace(-50,50,100)\n",
    "test = results[bkg_key]['target'][:,9:20,:]\n",
    "test = test.reshape(results[bkg_key]['target'].shape[0]*10,3)\n",
    "mask0 = test[:,0]!=0\n",
    "mask1 = test[:,1]!=0\n",
    "mask2 = test[:,2]!=0\n",
    "mask = mask0 + mask1 + mask2\n",
    "test = test[mask]\n",
    "predict = results[bkg_key]['prediction'][:,9:20,:]\n",
    "predict = predict.reshape(results[bkg_key]['prediction'].shape[0]*10,3)\n",
    "predict = predict[mask]\n",
    "plt.hist(test[:,0].flatten(), bins=bins_, alpha=0.5, label='X_test')\n",
    "plt.hist(predict[:,0].flatten(), bins=bins_, alpha=0.5, label='X_predict')\n",
    "plt.xlabel('Pi (GeV)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yscale('log')\n",
    "plt.title('Jet pt and MET') \n",
    "plt.savefig('{outdir}/reco_{model_type}_jetmet.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make roc curves\n",
    "signal_eff={}\n",
    "scale_factor_nugun = 11245.6*2544. #2e34*6.92e-26 # Hz/cm^2 * cm^2 https://twiki.cern.ch/twiki/bin/view/CMS/PileupJSONFileforData#Recommended_cross_section\n",
    "scale_factor_nugun = scale_factor_nugun/(10**6) # MHz\n",
    "\n",
    "print(\"MSE Loss:\")\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if key=='QCD': continue\n",
    "    if('TT' in key): key_ = 'TTbar'\n",
    "    if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "    if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "    if('TauTau' in key): key_ = 'SM H -> 2tau'\n",
    "    signal_eff[key]={}\n",
    "    true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "    pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "    fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "    roc_auc = auc(fpr_loss, tpr_loss)\n",
    "    rate = fpr_loss*scale_factor_nugun\n",
    "    signal_eff[key]['MSE_loss_5kHz']=tpr_loss[rate<0.005][-1]\n",
    "    signal_eff[key]['MSE_loss_10Hz']=tpr_loss[rate<1e-5][-1]\n",
    "    print(key_,\"%.2f\"%(tpr_loss[rate<0.005][-1]*100)+\"%\", '@ 5kHz')\n",
    "    print(key_,\"%.3f\"%(tpr_loss[rate<1e-5][-1]*100)+\"%\", '@ 10 Hz')\n",
    "    plt.plot(rate, tpr_loss, label=key_+' (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, scale_factor_nugun], [0, 1], 'k--')\n",
    "plt.xlim([1e-6, scale_factor_nugun])\n",
    "plt.ylim([1e-6, 1.05])\n",
    "plt.xlabel('Trigger Rate (MHz)')\n",
    "plt.ylabel('Signal Efficiency')\n",
    "plt.title('MSE Loss')\n",
    "plt.axvline(0.005, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(10.0/10.0**6, color='green', linestyle='dashed', linewidth=1)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig( '{outdir}/roc_curve_{model_type}_MSE_loss.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "if(model_type=='VAE'):\n",
    "    \n",
    "    print(\"\\nTotal Loss:\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'\n",
    "        true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        roc_auc = auc(fpr_loss, tpr_loss)\n",
    "        rate = fpr_loss*scale_factor_nugun\n",
    "        signal_eff[key]['Total_loss_5kHz']=tpr_loss[rate<0.005][-1]\n",
    "        signal_eff[key]['Total_loss_10Hz']=tpr_loss[rate<1e-5][-1]\n",
    "        print(key_,\"%.2f\"%(tpr_loss[rate<0.005][-1]*100)+\"%\", '@ 5kHz')\n",
    "        print(key_,\"%.6f\"%(tpr_loss[rate<1e-5][-1]*100)+\"%\", '@ 10 Hz')\n",
    "        plt.plot(rate, tpr_loss, label=key_+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, scale_factor_nugun], [0, 1], 'k--')\n",
    "    plt.xlim([1e-6, scale_factor_nugun])\n",
    "    plt.ylim([1e-6, 1.05])\n",
    "    plt.xlabel('Trigger Rate (MHz)')\n",
    "    plt.ylabel('Signal Efficiency')\n",
    "    plt.title('Total Loss')\n",
    "    plt.axvline(0.005, color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(10.0/10.0**6, color='green', linestyle='dashed', linewidth=1)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig( '{outdir}/roc_curve_{model_type}_Total_loss.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "    # plt.show()\n",
    "\n",
    "    print(\"\\nRadius:\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'\n",
    "        true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        roc_auc = auc(fpr_loss, tpr_loss)\n",
    "        rate = fpr_loss*scale_factor_nugun\n",
    "        signal_eff[key]['Radius_5kHz']=tpr_loss[rate<0.005][-1]\n",
    "        signal_eff[key]['Radius_10Hz']=tpr_loss[rate<1e-5][-1]\n",
    "        print(key_,\"%.2f\"%(tpr_loss[rate<0.005][-1]*100)+\"%\", '@ 5kHz')\n",
    "        print(key_,\"%.6f\"%(tpr_loss[rate<1e-5][-1]*100)+\"%\", '@ 10 Hz')\n",
    "        plt.plot(rate, tpr_loss, label=key_+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, scale_factor_nugun], [0, 1], 'k--')\n",
    "    plt.xlim([1e-6, scale_factor_nugun])\n",
    "    plt.ylim([1e-6, 1.05])\n",
    "    plt.xlabel('Trigger Rate (MHz)')\n",
    "    plt.ylabel('Signal Efficiency')\n",
    "    plt.title('Radius')\n",
    "    plt.axvline(0.005, color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(10.0/10.0**6, color='green', linestyle='dashed', linewidth=1)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig( '{outdir}/roc_curve_{model_type}_radius.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "    # plt.show()\n",
    "\n",
    "    print(\"\\nKL Loss:\")\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        if('TT' in key): key_ = 'TTbar'\n",
    "        if('haa' in key): key_ = 'H -> aa -> 4b, ma = 15 GeV'\n",
    "        if('HH' in key): key_ = 'SM HH -> 4b'\n",
    "        if('TauTau' in key): key_ = 'SM H -> 2tau'\n",
    "        true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['kl_loss'], results['QCD']['kl_loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        roc_auc = auc(fpr_loss, tpr_loss)\n",
    "        rate = fpr_loss*scale_factor_nugun\n",
    "        signal_eff[key]['KL_loss_5kHz']=tpr_loss[rate<0.005][-1]\n",
    "        signal_eff[key]['KL_loss_10Hz']=tpr_loss[rate<1e-5][-1]\n",
    "        print(key_,\"%.2f\"%(tpr_loss[rate<0.005][-1]*100)+\"%\", '@ 5kHz')\n",
    "        print(key_,\"%.6f\"%(tpr_loss[rate<1e-5][-1]*100)+\"%\", '@ 10 Hz')\n",
    "        plt.plot(rate, tpr_loss, label=key_+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, scale_factor_nugun], [0, 1], 'k--')\n",
    "    plt.xlim([1e-6, scale_factor_nugun])\n",
    "    plt.ylim([1e-6, 1.05])\n",
    "    plt.xlabel('Trigger Rate (MHz)')\n",
    "    plt.ylabel('Signal Efficiency')\n",
    "    plt.title('KL Loss')\n",
    "    plt.axvline(0.005, color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.axvline(10.0/10.0**6, color='green', linestyle='dashed', linewidth=1)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig( '{outdir}/roc_curve_{model_type}_kl_loss.pdf'.format(outdir=os.path.join(os.getcwd(),output_model_h5.split('.')[0]),model_type=model_type))\n",
    "    # plt.show()\n",
    "    \n",
    "signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_pd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
