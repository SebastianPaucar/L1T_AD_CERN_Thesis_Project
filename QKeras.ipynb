{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic heterogeneous quantization of DNN for low-latency inference on the edge for particle detectors\n",
    "\n",
    "<span style=\"color: red;\">  ¿What does \"low-latency inference\" means? </span>\n",
    "\n",
    "#### **There is two facts facing each other:**\n",
    "\n",
    "* More accurate solutions is pushing DL research towards more complex algorithms (_higher model size_).\n",
    "* Edge devices demand efficient inference (_model size_, latency and energy consumption _reduction_).\n",
    "\n",
    "#### **There is a possible solution for this contradiction:**\n",
    "\n",
    "* _Quantization_ limits the model size. It consists in using fewer bits to represent weights and biases. \n",
    "\n",
    "#### **But this possible solution has a problem:**\n",
    "\n",
    "* _Quantization_ usually results in a performance decline. \n",
    "\n",
    "#### **For this reason, we want to find (indeed, we already have found):**\n",
    "\n",
    "* A method for designing optimally heterogeneusly quantized version of DNN models for minimum-energy, high-accuracy, nanosecond inference and fully automated deployment on chip.\n",
    "\n",
    "#### ** And how we can minimize the energy consumption and model size while high accuracy is mantained?**\n",
    "\n",
    "* In order for that, we can use a per-layer and per-parameter type automatic quantization procedure, sampling for a wide range of quantizers.\n",
    "\n",
    "_That is crucial for the event selection in proton-proton collisions at CERN-LHC_\n",
    "\n",
    "At LHC, resources are limited and a latency of O(1) us is required. Nanosecond inference and a resource consumption reduced are achieved when its is implemented on a FPGA.\n",
    "\n",
    "#### **What is all this about?**\n",
    "\n",
    "There are two main ideas:\n",
    "\n",
    "* Real-time inference of DNNs on custom hardware has become increasingly relevant.\n",
    "* Typical acceptable latency for real-time inference is O(1)ms. There is other applications which require sub-us inference _¿why?_.\n",
    "\n",
    "<span style=\"color: red;\"> Key idea: ¿Why does we want low latency? </span>\n",
    "\n",
    "In HEP:\n",
    "\n",
    "* HEP is at the extreme inference spectrum of both the low-latency and limited-area.\n",
    "* In particular, proton-proton collisions data processing at LHC-CERN requires thoses conditions.\n",
    "\n",
    "#### **What is going on in the LHC?**\n",
    "\n",
    "* In its particle detectors, tens of  data terabytes per second are produced from collisions ocurring every 25 ns.\n",
    "* This data is reduced by _the trigger_ (a real-time event filter processing system).\n",
    "* The trigger decides whether a discrete collision event is able to analyze or not.\n",
    "\n",
    "#### **What about the trigger?**\n",
    "\n",
    "* Data is buffered while the processing occurs, with a O(1) us maximum latency to make the trigger decision.\n",
    "* High selection accuracy in the trigger is crucial to keep only the most interesting events  while keeping the output bandwidth low. This reduces the event reate from 40 MHz to 100kHz.\n",
    "\n",
    "_LHC will be upgraded to HL-LHC, increasing the collision rate by a factor of 5-7. This will result in a accumulated data total amount one order higher than LHC capabilities._\n",
    "\n",
    "_With this extreme increase, ML solutions are being explored as fast approximations of current algorithms in use to minimize the latency and maximize the precision of tasks._\n",
    "\n",
    "#### **What about the implementation?**\n",
    "\n",
    "* Real-time inference hardware in detectors has limited computational capacity due to size constraints. \n",
    "* Incorporating resource-intensive models without a loss in performance poses a great challenge.\n",
    "* Compact network design, weight and filter prunning or quantization are part of efficient inference development techniques.\n",
    "\n",
    "_Quantization-aware training solutions have been suggested_\n",
    "\n",
    "#### **What does Quantization-aware consist?**\n",
    "\n",
    "* A fixed numerical representation is adopted for the whole model. The model training is performed enforcing this constraint during weight optimization.\n",
    "\n",
    "_Some layers may be more accommodating for aggressive quantization, whereas others may require more expensive arithmetic._\n",
    "\n",
    "_Per-layer heterogeneous quantization is the optimal way to achieve higher accuracy at low resource cost. It might require further specialization of the hardware resources._\n",
    "\n",
    "#### **What do we want to develop?**\n",
    "\n",
    "* A novel workflow for finding the optimal heterogeneous quantization per layer and per parameter type for a given model.\n",
    "* Deploying that workflow on FPGA hardware.\n",
    "\n",
    "Also we expect:\n",
    "\n",
    "* Implement a range of quantization methods in a common library (that will be provide a broad base for optimal quantization easily sampling).\n",
    "* A novel method for optimal heterogeneous quantization finding for a given model, resulting in minimum area or power DNNs while maintaining high accuracy.\n",
    "\n",
    "#### **Indeed, we have _QKeras & AutoQKeras_ libraries:**\n",
    "\n",
    "* These libraries replace Keras layers, transforming Keras models to their equivalent deep heterogeneously quantized versions, which are trained quantization aware.\n",
    "* Using AutoQKeras, a user can trade-off accuracy by model size reduction.\n",
    "\n",
    "#### **Why is it important for HEP on edge applications?**\n",
    "\n",
    "* It can classify events in the proton-proton collisions triggering at CERN-LHC, where resoruces are limited and a maximum latency of O(1) us is imposed.\n",
    "* Inference within 60 ns and model resource consumption reduction by a factor of 50 can be achieved through heterogeneous quantization, while maintaining similar accuracy (within 3% of the floating point model accuracy).\n",
    "* It show that the original floating point model accuracy can be maintained for homogeneously quantized DNNs down to a bit width of 6 while reducing resource consumption up to 75% thorugh Qkeras traning quantization-aware.\n",
    "\n",
    "_Another ML goal is deploying ultra low latency and low-area DNNs on chip. That is crucial for the deployment of ML models on FPGAs in particle detectors and other fields with extreme inference and low power requirements._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
