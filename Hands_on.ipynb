{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***What is the goal?***\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:40:07` Okay, so the next step is for me to give you samples with additional inputs.\n",
    "\n",
    "`18:40:14` Then we compare the baseline training with the new baseline with the new model that uses your inputs.\n",
    "\n",
    "`18:40:22` Okay, I will just add other particles and other features...\n",
    "\n",
    "`18:40:28` ...hmm, so that's the goal.\n",
    "\n",
    "`18:40:35` Except that you need the files from me, which I will produce, or at least, yeah, because I just need to change the converter. I just need to add the new variables.\n",
    "\n",
    "**Diptarko Choudhury:**\n",
    "\n",
    "`18:41:59` ...the rest is about you, Sebastián, trying to understand how to add this new input to the model.\n",
    "\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:42:05` Because basically... I mean, we just have to expand the model.\n",
    "\n",
    "**Diptarko Choudhury:**\n",
    "\n",
    "`18:42:12` So I believe, Sebastián, you have the codes. So the thing is, the model codes and training codes are ready.\n",
    "\n",
    "`18:42:19` The only thing is I have to write the rate calculation function. Patrick is a little bit busy with summer, so either I do it because I think Sebastián will be needing the rate function pretty soon. Otherwise, he won't be able to verify his results.\n",
    "\n",
    "`18:42:33` So I think I will have to write that today, so that Sebastián can use it for now, like some function.\n",
    "\n",
    "`18:42:42` And I think this should be good to go, because you have the training scripts and everything, like you have the normal V model that Chang used, and everything like...\n",
    "\n",
    "`18:42:49` ...whatever that he was using, all the normalization, everything.\n",
    "\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:43:03` Okay, so now, Dip, the setup that we showed to Sebastián is the latest, right? So it trains the latest 0 bias. And the latest sample Monte Carlo signals?\n",
    "\n",
    "**Diptarko Choudhury:**\n",
    "\n",
    "`18:43:19` Yeah, so this is the same Monte Carlo signals that Chang gave me, like he said that he uses for the V4 model.\n",
    "\n",
    "`18:43:27` Yeah, so this is the same model. So the idea is, basically what...\n",
    "\n",
    "`18:43:32` ...the entire thing that I used for the V4.\n",
    "\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:43:41` Okay, yeah, so then I'll do the converter today, and then send it. I'll put it on GitHub and then StageMaker. Then I'll try merging that into the pipeline, and then open a PR into the master.\n",
    "\n",
    "**Diptarko Choudhury:**\n",
    "\n",
    "`18:43:56` Yeah, then, yeah, so by the end of Sebastián's time, like by the end of this month, we have everything on the master with the flag.\n",
    "\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:44:08` Yeah, and then let's try to get some results for the new training, like this week or early next week.\n",
    "\n",
    "**Jennifer Ngadiuba:**\n",
    "\n",
    "`18:44:47` Okay, so we still have the full next week to get some ROC curves with the new inputs.\n",
    "\n",
    "`18:44:55` And then some more... like to present with some validation plots.\n",
    "\n",
    "`18:45:01` Yeah, okay, so you should be working on this, and then we will continue during the week.\n",
    "\n",
    "`18:45:17` Sebastián, I will send you some things later this evening. I will prepare the converter and some of the scripts for the new inputs so that you can take them and start playing with the new models. So that we can try and get some plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Set up an environment using CERN SWAN***\n",
    "It is good note that how git was used for the following task:\n",
    "\n",
    "```bash\n",
    "[spaucarm@jupyter-spaucarm ~]$ git clone https://gitlab.cern.ch/cms-l1-ad/l1_anomaly_ae.git\n",
    "Cloning into 'l1_anomaly_ae'...\n",
    "warning: templates not found in /build/jenkins/workspace/lcg_release_pipeline/install/git/2.29.2/x86_64-el9-gcc13-opt/share/git-core/templates\n",
    "Username for 'https://gitlab.cern.ch': spaucarm\n",
    "Password for 'https://spaucarm@gitlab.cern.ch':\n",
    "remote: Enumerating objects: 10942, done.\n",
    "remote: Counting objects: 100% (487/487), done.\n",
    "remote: Compressing objects: 100% (486/486), done.\n",
    "remote: Total 10942 (delta 332), reused 0 (delta 0), pack-reused 10455\n",
    "Receiving objects: 100% (10942/10942), 125.08 MiB | 11.13 MiB/s, done.\n",
    "Resolving deltas: 100% (7541/7541), done.\n",
    "Updating files: 100% (75/75), done.\n",
    "[spaucarm@jupyter-spaucarm ~]$ git checkout refactory\n",
    "fatal: not a git repository (or any parent up to mount point /eos)\n",
    "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
    "[spaucarm@jupyter-spaucarm ~]$ ls -a\n",
    ".  ..  SWAN_projects  l1_anomaly_ae\n",
    "[spaucarm@jupyter-spaucarm ~]$ mv l1_anomaly_ae/ SWAN_projects/\n",
    ".notebook_nbextensions  demo/\n",
    "[spaucarm@jupyter-spaucarm ~]$ mv l1_anomaly_ae/ SWAN_projects/\n",
    ".notebook_nbextensions  demo/\n",
    "[spaucarm@jupyter-spaucarm ~]$ mv l1_anomaly_ae/ SWAN_projects/demo/\n",
    "[spaucarm@jupyter-spaucarm ~]$ cd SWAN_projects/demo/\n",
    "[spaucarm@jupyter-spaucarm demo]$ cd l1_anomaly_ae/\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ ls -a\n",
    ".  ..  .git  .gitignore  README.md  cnn  dnn  environment.yml  firmware  in  notebooks_plotting\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ git checkout refactory\n",
    "Branch 'refactory' set up to track remote branch 'refactory' from 'origin'.\n",
    "Switched to a new branch 'refactory'\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ ls -a\n",
    ".   .git            .gitignore  cnn  dnn_revamp       environment2.yml  in                  untitled.txt\n",
    "..  .gitattributes  README.md   dnn  environment.yml  firmware          notebooks_plotting\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ pip list\n",
    "Package                           Version            Editable project location\n",
    "--------------------------------- ------------------ -------------------------------------------------------------------------------------------\n",
    "absl-py                           1.4.0\n",
    "accelerate                        0.20.3\n",
    "AGILe                             1.5.0\n",
    "aiohttp                           3.8.4\n",
    "aiosignal                         1.2.0\n",
    ".\n",
    ".\n",
    ".\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ pip install qkeras\n",
    "Defaulting to user installation because normal site-packages is not writeable\n",
    "Requirement already satisfied: qkeras in /cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages (0.9.0)\n",
    ".\n",
    ".\n",
    ".\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ ls -a\n",
    ".   .git            .gitignore  cnn  dnn_revamp       environment2.yml  in                  untitled.txt\n",
    "..  .gitattributes  README.md   dnn  environment.yml  firmware          notebooks_plotting\n",
    "[spaucarm@jupyter-spaucarm l1_anomaly_ae]$ cd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***[Solved] Error when imports `neptunecontrib`***\n",
    "You must to make sure that the path of python packages are located in the `PYTHONPATH`. For this, you have to do the following (it worked on Jupyter notebook):\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('/eos/home-i02/s/spaucarm/.local/lib/python3.9/site-packages')\n",
    "```\n",
    "And you can verified that it worked through:\n",
    "```python\n",
    "print(sys.path)\n",
    "['/eos/home-i02/s/spaucarm/SWAN_projects/OldBaseline/dnn', '/cvmfs/sft.cern.ch/lcg/releases/condor/23.0.2-36b2d/x86_64-el9-gcc13-opt/lib/python3', '/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/itk', '/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/python', '/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib', '', '/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages', '/usr/local/lib/swan/nb_term_lib', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc13-opt/lib/python39.zip', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc13-opt/lib/python3.9', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc13-opt/lib/python3.9/lib-dynload', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc13-opt/lib/python3.9/site-packages', '/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/IPython/extensions', '/home/spaucarm/.ipython', '/eos/home-i02/s/spaucarm/.local/lib/python3.9/site-packages']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Still not Solved] Why does this error appear? I'm trying to split the data:\n",
    "```python\n",
    "X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, 'Cartesian', output_pfile,True)\n",
    "---------------------------------------------------------------------------\n",
    "PermissionError                           Traceback (most recent call last)\n",
    "/tmp/ipykernel_1309/2130387577.py in <module>\n",
    "----> 1 X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, 'Cartesian', output_pfile,True)\n",
    "\n",
    "/eos/home-i02/s/spaucarm/SWAN_projects/OldBaseline/dnn/data_preprocessing.py in prepare_data(input_file, input_bsm, events, norm_strategy, output_file, return_data)\n",
    "     18     L1bits_test = {}\n",
    "     19 \n",
    "---> 20     with h5py.File(input_file, 'r') as h5f:\n",
    "     21         # remove last feature, which is the type of particle\n",
    "     22         data = np.array(h5f['full_data_cyl'][:events,:,:])#.astype(np.float16)\n",
    "\n",
    "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\n",
    "    565                                  fs_persist=fs_persist, fs_threshold=fs_threshold,\n",
    "    566                                  fs_page_size=fs_page_size)\n",
    "--> 567                 fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
    "    568 \n",
    "    569             if isinstance(libver, tuple):\n",
    "\n",
    "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\n",
    "    229         if swmr and swmr_support:\n",
    "    230             flags |= h5f.ACC_SWMR_READ\n",
    "--> 231         fid = h5f.open(name, flags, fapl=fapl)\n",
    "    232     elif mode == 'r+':\n",
    "    233         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
    "\n",
    "h5py/_objects.pyx in h5py._objects.with_phil.wrapper()\n",
    "\n",
    "h5py/_objects.pyx in h5py._objects.with_phil.wrapper()\n",
    "\n",
    "h5py/h5f.pyx in h5py.h5f.open()\n",
    "\n",
    "PermissionError: [Errno 1] Unable to open file (unable to open file: name = '/eos/user/j/jngadiub/files-for-demo/ZB_preprocessed.h5', errno = 1, error message = 'Operation not permitted', flags = 0, o_flags = 0)\n",
    "```\n",
    "I can also note that:\n",
    "```python\n",
    "ls -ld /eos/user/j/jngadiub/files-for-demo\n",
    "drwxr-xr-x. 2 33877 1399 4096 Jul 19 16:29 /eos/user/j/jngadiub/files-for-demo/\n",
    "ls -ld /eos/user/j\n",
    "lrwxrwxrwx. 1 root root 15 Jul 22 09:31 /eos/user/j -> /eos/home-i01/j\n",
    "```\n",
    "In the same way, I'm facing with the following error (I was trying to view some data)\n",
    "```python\n",
    "head -n 10 /eos/user/j/jngadiub/files-for-demo/ZB_preprocessed.h5\n",
    "head: cannot open '/eos/user/j/jngadiub/files-for-demo/ZB_preprocessed.h5' for reading: Operation not permitted\n",
    "```\n",
    "Which seems to be an error due to lack of permissions, because at the moment of execute the following testing code, it shows an error:\n",
    "```python\n",
    "try:\n",
    "    with h5py.File(input_qcd, 'r') as h5f:\n",
    "        print(\"File opened successfully\")\n",
    "        print(list(h5f.keys()))\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "Error: [Errno 1] Unable to open file (unable to open file: name = '/eos/user/j/jngadiub/files-for-demo/ZB_preprocessed.h5', errno = 1, error message = 'Operation not permitted', flags = 0, o_flags = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Question about the goal: What branches should I use?***\n",
    "I understand that I have to compare the two training baselines, but my question is which gitlab branches do I have to use? I was studying the OldBaseline_KLL_MLT22 tag and the master branch. Are these the models I have to use? \n",
    "\n",
    "On the other hand, when using the End2End_demo.ipnyb of OldBaseline, in CERN SWAN, I have encountered this error, am I missing some permission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Instructions by Diptarko**\n",
    "**Diptarko Choudhury**\n",
    "\n",
    "`[18:17:00]:` We’ve got a lot of branches in the repository. To clarify, you’ll need to work with the refactored branch.\n",
    "\n",
    "`[18:17:13]:` Yes, so the refactored branch is where the latest code improvements are integrated. You won't be entering directly into the original DNN (Deep Neural Network) branch. Instead, you’ll use the refactored branch, which has the necessary updates.\n",
    "\n",
    "`[18:17:21]:` Great. Now, this is the code you need to run. The one you’ve been trying to run won't work because the environment is a bit broken. Instead, use this DN (Dynamic Neural) and Revamp code. It should work smoothly.\n",
    "\n",
    "`[18:17:54]:` First, you’ll need to run the \"digital_order.ipynb\" file. This will generate a dataset for you. After that, you can move on to running \"train.ipynb\", which should allow you to generate a model.\n",
    "\n",
    "**Sebastián Paucar Mendoza**\n",
    "\n",
    "`[18:18:16]:` Okay, I understand. But during the last meeting, Jennifer mentioned I need to compare baselines. I’m not entirely sure what she meant. One of them is the refactored version, right?\n",
    "\n",
    "**Diptarko Choudhury**\n",
    "\n",
    "`[18:18:37]:` Exactly. You’ll be comparing two baselines: one from the refactored inputs and another from the original branch before the refactor. The goal here is to ensure that the refactored code doesn’t degrade performance compared to the existing version. However, the performance comparison and the final results are something I’m handling, so you don't need to worry about that.\n",
    "\n",
    "`[18:19:07]:` Your focus should be on working with the refactored branch. You’ll need to create a new branch from there and make some modifications to the data utilization.\n",
    "\n",
    "`[18:19:16]:` Specifically, you’ll need to tweak the data files. Right now, you're reading in some newer files, and those might require slight adjustments. Also, the training file you’re working with currently processes 57 parameters based on 19 events, each contributing three basic parameters: petify, pt, etc. You might need to adjust this for more parameters depending on the new data.\n",
    "\n",
    "`[18:20:17]:` After updating the parameters, you should compare the results with the baseline to ensure everything is functioning correctly. But don’t worry about the more complex parts, like rate calculations—that’s on me.\n",
    "\n",
    "`[18:20:32]:` Once you’ve integrated the new data into the system, you’ll need to use the data loaders from the existing cases. There’s not much to change there, except perhaps for how files are being read. You might need to confirm with Jennifer exactly what additional data needs to be integrated.\n",
    "\n",
    "`[18:21:07]:` In my next presentation, I’ll provide a more detailed demo. For now, you’ll need to branch off, make the necessary modifications to the files, integrate your data loader, and then plug this into the existing models and loss functions.\n",
    "\n",
    "`[18:21:29]:` Remember, don’t touch any other branches. We’re moving away from some of the old code, especially the older DNN code.\n",
    "\n",
    "`[18:21:49]:` Sure. We’re transitioning away from the old end-to-end code structure. The goal is to establish this new DN and Revamp branch as our main framework. So your work should focus entirely on this branch, following the refactoring and updates.\n",
    "\n",
    "`[18:22:25]:` Jennifer has already made some updates, including the creation of new branches and conversion files. For example, she’s added new parameters like muon quality, jet HW (hardware), and tau properties. These are all reflected in the new \"convert_to_h5.py\" script, which generates HDF5 files for model training.\n",
    "\n",
    "`[18:22:49]:` The HDF5 files that are generated include data for both L1 trigger analysis and Monte Carlo simulations. The new pipeline she set up is almost ready. You’ll need to ensure the new data you’re working with is correctly read and processed by this pipeline.\n",
    "\n",
    "`[18:23:33]:` I’ll provide you with the files you need. Your task is to write the code that reads these files and processes them for the training.\n",
    "\n",
    "`[18:24:42]:` Also, there’s no need for specialized hardware or GPU configurations. You can install all the necessary packages directly from the command line using tools like Conda. Let me show you how to set that up.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "You need to change data, so this files needs to be changed a bit: `l1_anomaly_ae/dnn_revamp/data_util/_read.py`\n",
    "\n",
    "Then, training files using `l1_anomaly_ae/dnn_revamp/train.ipynb`. It will take a bit more number of parameters.\n",
    "\n",
    "Then compare the existing baseline (pointing again to `l1_anomaly_ae/dnn_revamp/train.ipynb`) with the results of a new parameters that you get.\n",
    "\n",
    "You need to integrate new data file to your existing data loaders. You might need to change the lines 20-24 part of `l1_anomaly_ae/dnn_revamp/data_util/_read.py`.\n",
    "\n",
    "I need to clarify with Jennifer what data she wants to add or she already added.\n",
    "\n",
    "Change this files a bit into a new branch (Diptarko is still pointing to `_read.py`) and get your data loader, and then plug that into the existing models an losses (in this moment, Diptarko is pointing to `l1_anomaly_ae/dnn_revamp`).\n",
    "\n",
    "I don't need to touch any other branch, we are trying to move away from the existing code. I need to work on the `revamp` file.\n",
    "\n",
    "Jennifer have created a new branch where she changed the H5 files converter (Diptarko is now pointing at `/l1ntuple-maker/convert_to_h5.py`, at the `133XWinter24_newInputs` branch). She added new stuffs, now there are new quantities that she has already added. The result of that new converter I will need to use to train the model.\n",
    "\n",
    "This creates two files, one H5 files for L1 anomalies (0 biased), and an other H5 files for Monte Carlo simulation. The thing is getting this data read. I will have to write the parts.\n",
    "\n",
    "Diptarko's CERN SWAN configuration: 105 CUDA 11.8.89 (GPU)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
